"""
Transcription Page
Ch·∫°y ASR v·ªõi Whisper, chunking nh·∫π, t·ªëi ∆∞u cho Streamlit Cloud
"""
import streamlit as st
import os
import sys
import tempfile
import re
import time
import soundfile as sf

# ================== PATH ==================
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from app.components.layout import apply_custom_css, render_page_header
from app.components.transcript_editor import render_transcript_editor
from app.components.footer import render_footer

from core.asr.model_registry import (
    get_all_models,
    get_model_info,
    check_model_dependencies,
    get_recommended_models,
)
from core.asr.transcription_service import (
    load_whisper_model,
    transcribe_audio,
    split_text_readable,
    format_time,
)
from core.nlp.post_processing import normalize_vietnamese, format_text
from core.asr.quality_presets import (
    get_model_size_for_preset,
    get_preset_description,
    get_preset_tooltip,
    get_recommended_preset,
    get_all_presets,
    detect_gpu,
)
from core.audio.audio_processor import chunk_signal
from core.asr.transcription_service import format_time
from core.audio.ffmpeg_setup import ensure_ffmpeg

# ================== ENV ==================
ensure_ffmpeg(silent=True)
apply_custom_css()

st.set_page_config(
    page_title="Transcription - Vietnamese Speech to Text",
    page_icon="üìù",
    layout="wide",
)

# ================== STATE ==================
def init_state():
    defaults = {
        "audio_data": None,
        "audio_sr": None,
        "audio_info": None,
        "transcript_text": "",
        "transcript_segments": [],
    }
    for k, v in defaults.items():
        st.session_state.setdefault(k, v)

init_state()

# ================== HEADER ==================
render_page_header("Transcription", "Ch·∫°y ASR v·ªõi Whisper, h·ªó tr·ª£ audio d√†i", "üìù")

# ================== GUARD ==================
if st.session_state.audio_data is None:
    st.warning("‚ö†Ô∏è Ch∆∞a c√≥ audio. Vui l√≤ng upload ·ªü trang Audio Input.")
    if st.button("üé§ Go to Audio Input", type="primary"):
        st.switch_page("pages/1_üé§_Audio_Input.py")
    st.stop()

st.info(
    f"üìä Duration: {st.session_state.audio_info['duration']:.1f}s | "
    f"SR: {st.session_state.audio_sr} Hz"
)

# ================== MODEL SELECTION ==================
st.subheader("üéØ Ch·ªçn M√¥ H√¨nh")

all_models = get_all_models()
recommended = set(get_recommended_models())

# Simplify: Only show recommended models by default, hide others in expander
recommended_model_ids = [mid for mid in all_models.keys() if mid in recommended]
other_model_ids = [mid for mid in all_models.keys() if mid not in recommended]

if recommended_model_ids:
    # Default to first recommended model
    default_index = 0
    selected_model_id = st.selectbox(
        "M√¥ h√¨nh ASR (Khuy·∫øn ngh·ªã)",
        recommended_model_ids,
        index=default_index,
        format_func=lambda mid: all_models[mid]["name"] + " üåü",
        help="Ch·ªçn m√¥ h√¨nh ASR. Whisper l√† l·ª±a ch·ªçn t·ªët nh·∫•t cho ti·∫øng Vi·ªát."
    )
    
    # Show other models in expander
    if other_model_ids:
        with st.expander("üîß M√¥ h√¨nh kh√°c (Kh√¥ng khuy·∫øn ngh·ªã)"):
            other_selected = st.selectbox(
                "M√¥ h√¨nh kh√°c",
                other_model_ids,
                format_func=lambda mid: all_models[mid]["name"],
                help="C√°c m√¥ h√¨nh n√†y c√≥ th·ªÉ kh√¥ng t·ªëi ∆∞u cho ti·∫øng Vi·ªát"
            )
            if st.button("S·ª≠ d·ª•ng m√¥ h√¨nh n√†y", key="use_other_model"):
                selected_model_id = other_selected
                st.rerun()
else:
    # Fallback if no recommended models
    model_ids = list(all_models.keys())
    selected_model_id = st.selectbox(
        "ASR Model",
        model_ids,
        format_func=lambda mid: all_models[mid]["name"],
        help="Ch·ªçn m√¥ h√¨nh ASR: Whisper (ƒëa ng√¥n ng·ªØ, h·ªó tr·ª£ ti·∫øng Vi·ªát)"
    )

model_info = get_model_info(selected_model_id)

is_available, missing = check_model_dependencies(selected_model_id)

if not is_available:
    st.error(f"‚ùå Thi·∫øu dependencies: {', '.join(missing)}")
    st.info("üí° **G·ª£i √Ω**: C√†i ƒë·∫∑t dependencies b·∫±ng l·ªánh: `pip install {' '.join(missing)}`")

# ================== QUALITY PRESET ==================
st.subheader("‚ö° Ch·ªçn Ch·∫•t L∆∞·ª£ng")

# Get recommended preset (auto-suggest Accurate if GPU available)
recommended_preset = get_recommended_preset(selected_model_id)
has_gpu = detect_gpu()

if has_gpu:
    st.success("üéÆ ƒê√£ ph√°t hi·ªán GPU! Khuy·∫øn ngh·ªã s·ª≠ d·ª•ng 'Ch√≠nh x√°c' ƒë·ªÉ c√≥ k·∫øt qu·∫£ t·ªët nh·∫•t.")

preset_options = get_all_presets()
preset_labels = {
    "fast": "‚ö° Nhanh - X·ª≠ l√Ω nhanh, ƒë·ªô ch√≠nh x√°c th·∫•p h∆°n",
    "balanced": "‚öñÔ∏è C√¢n b·∫±ng - T·ªëc ƒë·ªô v√† ƒë·ªô ch√≠nh x√°c c√¢n b·∫±ng (Khuy·∫øn ngh·ªã)",
    "accurate": "üéØ Ch√≠nh x√°c - X·ª≠ l√Ω ch·∫≠m h∆°n, ƒë·ªô ch√≠nh x√°c cao nh·∫•t"
}

selected_preset = st.radio(
    "Ch·ªçn ch·∫•t l∆∞·ª£ng x·ª≠ l√Ω:",
    preset_options,
    index=preset_options.index(recommended_preset) if recommended_preset in preset_options else 1,  # Default to balanced
    format_func=lambda p: preset_labels.get(p, p),
    help="Ch·∫ø ƒë·ªô 'C√¢n b·∫±ng' l√† l·ª±a ch·ªçn t·ªët nh·∫•t cho h·∫ßu h·∫øt tr∆∞·ªùng h·ª£p"
)

# Show description
st.info(f"üí° {get_preset_description(selected_preset)}")

# Auto-map preset to model size (hidden from user)
model_size = get_model_size_for_preset(selected_preset, selected_model_id)

if model_size is None:
    st.error(f"‚ùå K·∫øt h·ª£p preset/model kh√¥ng h·ª£p l·ªá")
    st.stop()

# Show technical details in expander (hidden by default)
with st.expander("‚ÑπÔ∏è Chi ti·∫øt k·ªπ thu·∫≠t", expanded=False):
    st.write(f"**K√≠ch th∆∞·ªõc model:** {model_size}")
    st.write(f"**Preset:** {selected_preset}")
    st.caption("üí° Th√¥ng tin k·ªπ thu·∫≠t chi ti·∫øt c√≥ th·ªÉ t√¨m th·∫•y trong Advanced Settings")

# Default options (hidden from regular users, moved to Advanced Settings)
enable_chunk = True  # Always enabled for long audio
chunk_seconds = 45  # Default chunk length
show_timestamps = True  # Always show timestamps

# ================== TRANSCRIBE ==================
def safe_get_text(result, default=""):
    """
    Safely extract text from transcription result.
    Handles None result (common on local when audio loading fails).
    
    Args:
        result: Transcription result dict or None
        default: Default text if result is None or missing 'text' key
    
    Returns:
        str: Extracted text or default
    """
    if result is None:
        return default
    if isinstance(result, dict):
        return result.get("text", default)
    # If result is already a string (shouldn't happen, but handle gracefully)
    if isinstance(result, str):
        return result
    return default

def run_chunked_transcription(run_fn):
    ranges = (
        chunk_signal(st.session_state.audio_data, st.session_state.audio_sr, chunk_seconds)
        if enable_chunk else [(0, len(st.session_state.audio_data))]
    )

    results = []
    progress_bar = st.progress(0.0)
    status_text = st.empty()
    error_count = 0
    temp_files_to_cleanup = []  # Track files to cleanup after transcription

    for i, (s0, s1) in enumerate(ranges, 1):
        y = st.session_state.audio_data[s0:s1]
        tmp_name = None

        try:
            # Create temp file and ensure it's closed before writing
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                tmp_name = tmp.name
            
            # Write audio data to file
            sf.write(tmp_name, y, st.session_state.audio_sr)
            
            # CRITICAL: Ensure file is flushed and closed before passing to transcription
            # On Windows, file must be fully written and closed before other processes can access it
            time.sleep(0.05)  # Small delay to ensure file system sync on Windows
            
            # Verify file exists and is readable before transcription
            if not os.path.exists(tmp_name):
                error_count += 1
                st.warning(f"‚ö†Ô∏è Chunk {i}/{len(ranges)}: Temp file kh√¥ng t·ªìn t·∫°i sau khi t·∫°o: {tmp_name}")
                continue
            
            # Normalize path for Windows (resolve any path issues)
            tmp_name_normalized = os.path.normpath(os.path.abspath(tmp_name))
            
            # Verify normalized path exists
            if not os.path.exists(tmp_name_normalized):
                error_count += 1
                st.warning(f"‚ö†Ô∏è Chunk {i}/{len(ranges)}: Normalized path kh√¥ng t·ªìn t·∫°i: {tmp_name_normalized}")
                # Try original path
                tmp_name_normalized = tmp_name
            
            # Run transcription
            result = run_fn(tmp_name_normalized)
            
            if result is None:
                error_count += 1
                continue
            
            # L·∫•y segments t·ª´ result v√† chia l·∫°i cho d·ªÖ ƒë·ªçc
            segments = result.get("segments", [])
            
            if segments:
                # T√≠nh chunk start time (absolute) - offset t·ª´ ƒë·∫ßu audio file
                chunk_start_time = s0 / st.session_state.audio_sr
                
                # X·ª≠ l√Ω t·ª´ng segment g·ªëc t·ª´ Whisper v√† chia l·∫°i cho d·ªÖ ƒë·ªçc
                chunk_results = []
                for original_seg in segments:
                    seg_start = original_seg.get("start", 0)  # Timestamp t∆∞∆°ng ƒë·ªëi trong chunk
                    seg_end = original_seg.get("end", 0)      # Timestamp t∆∞∆°ng ƒë·ªëi trong chunk
                    seg_text = original_seg.get("text", "").strip()
                    
                    if not seg_text:
                        continue
                    
                    # Chia text c·ªßa segment n√†y th√†nh c√°c ƒëo·∫°n nh·ªè d·ªÖ ƒë·ªçc (7-15 t·ª´, ‚â§2 c√¢u)
                    sub_texts = split_text_readable(seg_text, max_words=15, max_sentences=2)
                    
                    if not sub_texts:
                        continue
                    
                    # T√≠nh th·ªùi gian cho m·ªói ƒëo·∫°n con (chia ƒë·ªÅu th·ªùi gian)
                    seg_duration = seg_end - seg_start
                    num_parts = len(sub_texts)
                    per_part = seg_duration / num_parts if num_parts > 0 else seg_duration
                    
                    # T·∫°o readable segments v·ªõi timestamps ch√≠nh x√°c
                    for i, sub_text in enumerate(sub_texts):
                        # Timestamp t∆∞∆°ng ƒë·ªëi trong segment g·ªëc
                        sub_start = seg_start + i * per_part
                        sub_end = seg_start + (i + 1) * per_part
                        
                        # √Åp d·ª•ng post-processing cho ti·∫øng Vi·ªát
                        processed_text = normalize_vietnamese(sub_text)
                        processed_text = format_text(processed_text, {
                            "improve_vietnamese": True,
                            "punctuation": True,
                            "capitalize": True,
                            "remove_extra_spaces": True
                        })
                        
                        if processed_text.strip():
                            if show_timestamps:
                                # Timestamp absolute t·ª´ ƒë·∫ßu audio file
                                abs_start = chunk_start_time + sub_start
                                abs_end = chunk_start_time + sub_end
                                ts = f"[{format_time(abs_start)} - {format_time(abs_end)}] "
                            else:
                                ts = ""
                            
                            chunk_results.append(ts + processed_text.strip())
                
                if chunk_results:
                    results.extend(chunk_results)
            else:
                # Fallback: d√πng text n·∫øu kh√¥ng c√≥ segments
                text = safe_get_text(result)
                if text:
                    # √Åp d·ª•ng post-processing cho ti·∫øng Vi·ªát
                    text = normalize_vietnamese(text)
                    text = format_text(text, {
                        "improve_vietnamese": True,
                        "punctuation": True,
                        "capitalize": True,
                        "remove_extra_spaces": True
                    })
                    
                    if show_timestamps:
                        ts = f"[{format_time(s0 / st.session_state.audio_sr)} - {format_time(s1 / st.session_state.audio_sr)}] "
                    else:
                        ts = ""
                    results.append(ts + text.strip())
        except Exception as chunk_err:
            error_count += 1
            error_msg = str(chunk_err)
            if "WinError 2" in error_msg or "cannot find the file" in error_msg.lower():
                st.warning(f"‚ö†Ô∏è Chunk {i}/{len(ranges)}: File kh√¥ng t√¨m th·∫•y. C√≥ th·ªÉ file ƒë√£ b·ªã x√≥a ho·∫∑c path c√≥ v·∫•n ƒë·ªÅ.")
            else:
                st.warning(f"‚ö†Ô∏è Chunk {i}/{len(ranges)} failed: {error_msg}")
        finally:
            # Add to cleanup list instead of deleting immediately
            # This ensures file exists during entire transcription process
            if tmp_name and os.path.exists(tmp_name):
                temp_files_to_cleanup.append(tmp_name)

        # Update progress with detailed status
        progress_percent = i / len(ranges)
        progress_bar.progress(progress_percent)
        status_text.text(f"ƒêang x·ª≠ l√Ω ƒëo·∫°n {i}/{len(ranges)} ({progress_percent*100:.0f}%)...")

    # Cleanup all temp files after all transcriptions are complete
    for tmp_file in temp_files_to_cleanup:
        try:
            if os.path.exists(tmp_file):
                # Small delay to ensure file is not in use
                time.sleep(0.1)
                os.unlink(tmp_file)
        except Exception as cleanup_err:
            # Ignore cleanup errors - file may already be deleted
            pass

    # Clear status text
    status_text.empty()
    
    if error_count > 0 and len(results) == 0:
        # All chunks failed
        raise Exception(f"T·∫•t c·∫£ {error_count} ƒëo·∫°n x·ª≠ l√Ω ƒë·ªÅu th·∫•t b·∫°i. Vui l√≤ng ki·ªÉm tra file audio v√† model.")

    return "\n".join(results) if results else ""


if st.button("üöÄ Start Transcription", type="primary", use_container_width=True):
    if not is_available:
        st.stop()

    with st.spinner("Running ASR..."):
        try:
            if selected_model_id == "whisper":
                model, device = load_whisper_model(model_size)
                if model is None:
                    st.error("‚ùå Kh√¥ng th·ªÉ load Whisper model. Vui l√≤ng ki·ªÉm tra l·ªói ·ªü tr√™n.")
                    st.stop()
                # S·ª≠ d·ª•ng t·ªëi ∆∞u cho ti·∫øng Vi·ªát (default: enabled)
                text = run_chunked_transcription(
                    lambda p: transcribe_audio(
                        model, 
                        p, 
                        language="vi",
                        use_vietnamese_optimization=True  # T·ª± ƒë·ªông √°p d·ª•ng initial prompt v√† t·ªëi ∆∞u
                    )
                )
            else:
                st.error("‚ùå Unsupported model")
                st.stop()

            if not text or text.strip() == "":
                st.warning("‚ö†Ô∏è Transcription completed but result is empty. Check audio file and try again.")
            else:
                st.session_state.transcript_text = text
                st.success("‚úÖ Transcription complete")
                st.rerun()

        except Exception as e:
            error_msg = str(e)
            st.error(f"‚ùå Transcription th·∫•t b·∫°i: {error_msg}")
            
            # Provide helpful context for common errors with better formatting
            error_help = st.container()
            with error_help:
                if "NoneType" in error_msg or "None" in error_msg:
                    st.warning("""
                    **üí° L·ªói NoneType - Nguy√™n nh√¢n th∆∞·ªùng g·∫∑p:**
                    - Audio file kh√¥ng th·ªÉ load (ki·ªÉm tra format v√† path)
                    - Model kh√¥ng ƒë∆∞·ª£c load th√†nh c√¥ng
                    - FFmpeg kh√¥ng t√¨m th·∫•y ho·∫∑c kh√¥ng ho·∫°t ƒë·ªông
                    
                    **üîß C√°ch kh·∫Øc ph·ª•c:**
                    1. Quay l·∫°i trang Audio Input ƒë·ªÉ ki·ªÉm tra audio file
                    2. Xem l·ªói ·ªü tr√™n ƒë·ªÉ bi·∫øt model c√≥ load th√†nh c√¥ng kh√¥ng
                    3. Ki·ªÉm tra FFmpeg setup trong System Status
                    """)
                elif "Failed to load audio" in error_msg or "load audio" in error_msg.lower():
                    st.warning("""
                    **üí° L·ªói load audio - Nguy√™n nh√¢n th∆∞·ªùng g·∫∑p:**
                    - File format kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£
                    - File b·ªã h·ªèng ho·∫∑c kh√¥ng h·ª£p l·ªá
                    - FFmpeg kh√¥ng t√¨m th·∫•y ho·∫∑c kh√¥ng ho·∫°t ƒë·ªông
                    
                    **üîß C√°ch kh·∫Øc ph·ª•c:**
                    1. Th·ª≠ upload l·∫°i audio file ·ªü trang Audio Input
                    2. Ki·ªÉm tra format file (WAV, MP3, FLAC, M4A, OGG)
                    3. ƒê·∫£m b·∫£o file kh√¥ng b·ªã h·ªèng
                    4. Ki·ªÉm tra FFmpeg setup
                    """)
                elif "memory" in error_msg.lower() or "out of memory" in error_msg.lower():
                    st.warning("""
                    **üí° L·ªói b·ªô nh·ªõ - File audio qu√° l·ªõn:**
                    
                    **üîß C√°ch kh·∫Øc ph·ª•c:**
                    1. Chia nh·ªè file audio th√†nh c√°c ƒëo·∫°n ng·∫Øn h∆°n
                    2. S·ª≠ d·ª•ng preset 'Nhanh' thay v√¨ 'Ch√≠nh x√°c'
                    3. Gi·∫£m k√≠ch th∆∞·ªõc model (ch·ªçn 'tiny' ho·∫∑c 'base')
                    """)
                elif "cuda" in error_msg.lower() or "gpu" in error_msg.lower():
                    st.info("""
                    **üí° L·ªói GPU - H·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông chuy·ªÉn sang CPU:**
                    - N·∫øu c√≥ GPU, ki·ªÉm tra CUDA installation
                    - N·∫øu kh√¥ng c√≥ GPU, h·ªá th·ªëng s·∫Ω s·ª≠ d·ª•ng CPU (ch·∫≠m h∆°n)
                    """)
                else:
                    with st.expander("üîç Chi ti·∫øt l·ªói"):
                        st.exception(e)

# ================== OUTPUT ==================
if st.session_state.transcript_text:
    st.divider()
    st.subheader("üìù Transcript")

    st.text_area(
        "Result",
        st.session_state.transcript_text,
        height=300,
        disabled=True,
    )

    edited_text, _ = render_transcript_editor(
        st.session_state.transcript_text,
        key_prefix="transcript",
    )

    if st.button("üíæ Save edits"):
        st.session_state.transcript_text = edited_text
        st.success("Saved")
        st.rerun()

    st.divider()
    col1, col2 = st.columns(2)
    with col1:
        if st.button("‚ú® Speaker & Enhancement", use_container_width=True):
            st.switch_page("pages/3_‚ú®_Speaker_Enhancement.py")
    with col2:
        if st.button("üìä Export & Report", use_container_width=True):
            st.switch_page("pages/4_üìä_Export_Reporting.py")

# ===== Footer =====
render_footer()
