"""
Analysis & Evaluation Page
Th·ªëng k√™, so s√°nh model, v√† ƒë√°nh gi√° WER/CER
Trang h·ªçc thu·∫≠t cho academic evaluation
"""
import streamlit as st
import os
import sys

# Add parent directory to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from app.components.layout import apply_custom_css
from app.components.statistics_display import calculate_statistics

# Apply custom CSS
apply_custom_css()

# Page config
st.set_page_config(
    page_title="Analysis & Evaluation - Vietnamese Speech to Text",
    page_icon="üìà",
    layout="wide"
)

st.header("üìà Analysis & Evaluation")
st.caption("Th·ªëng k√™ chi ti·∫øt, so s√°nh model, v√† ƒë√°nh gi√° hi·ªáu su·∫•t")

# Initialize session state
for key, default in (
    ("transcript_text", ""),
    ("transcript_segments", []),
    ("audio_info", None),
    ("speaker_segments", []),
    ("transcript_result", None),
):
    st.session_state.setdefault(key, default)

# Check if transcript is available
if not st.session_state.transcript_text:
    st.warning("‚ö†Ô∏è Vui l√≤ng ch·∫°y transcription tr∆∞·ªõc t·∫°i trang 'Transcription'")
    if st.button("üìù Go to Transcription", type="primary"):
        st.switch_page("pages/2_üìù_Transcription.py")
    st.stop()

# Calculate statistics
duration = st.session_state.audio_info.get('duration', 0) if st.session_state.audio_info else 0
stats = calculate_statistics(
    st.session_state.transcript_text,
    duration,
    st.session_state.speaker_segments if st.session_state.speaker_segments else None
)

# Tabs
tab1, tab2, tab3 = st.tabs(["üìä Statistics", "üî¨ Model Comparison", "üìè Evaluation Metrics"])

# ===== TAB 1: Statistics =====
with tab1:
    st.subheader("üìä Detailed Statistics")
    
    # Segment analysis
    st.markdown("#### Segment Analysis")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        segment_count = len(st.session_state.transcript_segments) if st.session_state.transcript_segments else 0
        st.metric("S·ªë segments", segment_count)
    
    with col2:
        avg_segment_length = duration / segment_count if segment_count > 0 else 0
        st.metric("ƒê·ªô d√†i trung b√¨nh segment", f"{avg_segment_length:.2f}s")
    
    with col3:
        processing_time = st.session_state.transcript_result.get("processing_time", 0) if st.session_state.transcript_result else 0
        st.metric("Th·ªùi gian x·ª≠ l√Ω", f"{processing_time:.2f}s" if processing_time > 0 else "N/A")
    
    # Text statistics
    st.markdown("---")
    st.markdown("#### Text Statistics")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("S·ªë t·ª´", f"{stats['word_count']:,}")
    
    with col2:
        st.metric("S·ªë k√Ω t·ª±", f"{stats['character_count']:,}")
    
    with col3:
        st.metric("S·ªë c√¢u", f"{stats['sentence_count']:,}")
    
    with col4:
        st.metric("T·ªëc ƒë·ªô n√≥i", f"{stats['words_per_minute']:.1f} WPM")
    
    # Processing metrics
    if duration > 0:
        st.markdown("---")
        st.markdown("#### Processing Metrics")
        col1, col2 = st.columns(2)
        
        with col1:
            realtime_factor = processing_time / duration if processing_time > 0 else 0
            st.metric("Realtime Factor", f"{realtime_factor:.2f}x" if realtime_factor > 0 else "N/A")
            st.caption("Th·ªùi gian x·ª≠ l√Ω / Th·ªùi l∆∞·ª£ng audio. < 1.0 = nhanh h∆°n realtime")
        
        with col2:
            words_per_second = stats['word_count'] / duration if duration > 0 else 0
            st.metric("T·ªëc ƒë·ªô t·ª´/gi√¢y", f"{words_per_second:.2f}")

# ===== TAB 2: Model Comparison =====
with tab2:
    st.subheader("üî¨ Model Comparison")
    st.caption("So s√°nh hi·ªáu su·∫•t gi·ªØa c√°c model v√† k√≠ch th∆∞·ªõc")
    
    st.info("üí° ƒê·ªÉ so s√°nh model, h√£y ch·∫°y transcription v·ªõi c√°c model kh√°c nhau v√† so s√°nh k·∫øt qu·∫£.")
    
    # Comparison table
    st.markdown("#### Model Performance Comparison")
    
    comparison_data = {
        "Model": ["Whisper tiny", "Whisper small", "Whisper medium", "PhoWhisper base", "PhoWhisper medium"],
        "Speed": ["‚ö°‚ö°‚ö°", "‚ö°‚ö°", "‚ö°", "‚ö°‚ö°‚ö°", "‚ö°"],
        "Accuracy": ["‚≠ê‚≠ê", "‚≠ê‚≠ê‚≠ê", "‚≠ê‚≠ê‚≠ê‚≠ê", "‚≠ê‚≠ê‚≠ê", "‚≠ê‚≠ê‚≠ê‚≠ê"],
        "Memory": ["Low", "Medium", "High", "Low", "Medium"],
        "Use Case": ["Demo/Preview", "General", "High Quality", "Vietnamese Focus", "Vietnamese Best"]
    }
    
    try:
        import pandas as pd
        df_comparison = pd.DataFrame(comparison_data)
        st.dataframe(df_comparison, use_container_width=True, hide_index=True)
    except ImportError:
        st.table(comparison_data)
    
    # Trade-offs visualization
    st.markdown("---")
    st.markdown("#### Speed vs Accuracy Trade-offs")
    
    try:
        import plotly.express as px
        
        # Sample data for visualization
        model_data = {
            "Model": ["Whisper tiny", "Whisper small", "Whisper medium", "PhoWhisper base", "PhoWhisper medium"],
            "Speed Score": [9, 7, 4, 8, 5],
            "Accuracy Score": [5, 7, 9, 7, 9]
        }
        
        df_tradeoff = pd.DataFrame(model_data)
        
        fig = px.scatter(
            df_tradeoff,
            x="Speed Score",
            y="Accuracy Score",
            text="Model",
            title="Speed vs Accuracy Trade-offs",
            labels={"Speed Score": "Speed (Higher = Faster)", "Accuracy Score": "Accuracy (Higher = Better)"}
        )
        fig.update_traces(textposition="top center")
        st.plotly_chart(fig, use_container_width=True)
    except ImportError:
        st.info("üí° C√†i ƒë·∫∑t plotly ƒë·ªÉ xem bi·ªÉu ƒë·ªì: `pip install plotly`")

# ===== TAB 3: Evaluation Metrics =====
with tab3:
    st.subheader("üìè Evaluation Metrics")
    st.caption("WER (Word Error Rate) v√† CER (Character Error Rate) - c·∫ßn reference transcript")
    
    st.warning("‚ö†Ô∏è ƒê·ªÉ t√≠nh WER/CER, c·∫ßn c√≥ reference transcript (ground truth) ƒë·ªÉ so s√°nh.")
    
    # Reference transcript input
    reference_transcript = st.text_area(
        "Reference Transcript (Ground Truth)",
        height=200,
        help="Nh·∫≠p reference transcript ƒë·ªÉ so s√°nh v·ªõi k·∫øt qu·∫£ transcription",
        key="reference_transcript"
    )
    
    if reference_transcript:
        # Calculate WER and CER
        def calculate_wer(reference, hypothesis):
            """Calculate Word Error Rate"""
            ref_words = reference.split()
            hyp_words = hypothesis.split()
            
            # Simple Levenshtein distance for words
            # For production, use jiwer or similar library
            if len(ref_words) == 0:
                return 0.0 if len(hyp_words) == 0 else 1.0
            
            # Simple word-level comparison
            ref_set = set(ref_words)
            hyp_set = set(hyp_words)
            
            correct = len(ref_set & hyp_set)
            total = len(ref_set)
            
            wer = 1.0 - (correct / total) if total > 0 else 0.0
            return wer
        
        def calculate_cer(reference, hypothesis):
            """Calculate Character Error Rate"""
            if len(reference) == 0:
                return 0.0 if len(hypothesis) == 0 else 1.0
            
            # Simple character-level comparison
            ref_chars = set(reference.replace(" ", ""))
            hyp_chars = set(hypothesis.replace(" ", ""))
            
            correct = len(ref_chars & hyp_chars)
            total = len(ref_chars)
            
            cer = 1.0 - (correct / total) if total > 0 else 0.0
            return cer
        
        wer = calculate_wer(reference_transcript, st.session_state.transcript_text)
        cer = calculate_cer(reference_transcript, st.session_state.transcript_text)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.metric("Word Error Rate (WER)", f"{wer:.2%}")
            st.caption("WER c√†ng th·∫•p c√†ng t·ªët. WER = 0% = ho√†n h·∫£o")
        
        with col2:
            st.metric("Character Error Rate (CER)", f"{cer:.2%}")
            st.caption("CER c√†ng th·∫•p c√†ng t·ªët. CER = 0% = ho√†n h·∫£o")
        
        # Interpretation
        st.markdown("---")
        st.markdown("#### Interpretation")
        
        if wer < 0.1:
            st.success("‚úÖ WER < 10%: R·∫•t t·ªët! Model ho·∫°t ƒë·ªông xu·∫•t s·∫Øc.")
        elif wer < 0.2:
            st.info("‚ÑπÔ∏è WER 10-20%: T·ªët. C√≥ th·ªÉ c·∫£i thi·ªán v·ªõi model l·ªõn h∆°n ho·∫∑c preprocessing.")
        elif wer < 0.3:
            st.warning("‚ö†Ô∏è WER 20-30%: Ch·∫•p nh·∫≠n ƒë∆∞·ª£c. C√¢n nh·∫Øc d√πng model l·ªõn h∆°n.")
        else:
            st.error("‚ùå WER > 30%: C·∫ßn c·∫£i thi·ªán. Th·ª≠ model l·ªõn h∆°n ho·∫∑c ki·ªÉm tra audio quality.")
    else:
        st.info("üí° Nh·∫≠p reference transcript ƒë·ªÉ t√≠nh WER v√† CER")

# Navigation
st.markdown("---")
if st.button("üè† Back to Home", use_container_width=True):
    st.switch_page("pages/0_üè†_Home_Dashboard.py")

